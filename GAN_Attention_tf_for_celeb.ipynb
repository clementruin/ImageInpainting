{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN Architecture with Attention -- Tensorflow implementation - CELEB FACES\n",
    "\n",
    "PENDING\n",
    "- which losses for intermediary (approx) network?\n",
    "- size of filter is weird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import keras\n",
    "import pickle\n",
    "import os\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.layers import Dense, Dropout, Input, Conv2D, Conv2DTranspose, Flatten, Reshape\n",
    "from keras.models import Model,Sequential\n",
    "from tqdm import tqdm\n",
    "from keras.layers.advanced_activations import LeakyReLU, ReLU\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces_mask_path = Path('./datasets/faces_pp/faces_masks.pickle')\n",
    "faces_original_path = Path('./datasets/faces_pp/faces_original.pickle')\n",
    "faces_filter_path = Path('./datasets/faces_pp/faces_filter.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 3, 64, 64)\n"
     ]
    }
   ],
   "source": [
    "with open(faces_mask_path, 'rb') as file:\n",
    "    X_mask = pickle.load(file)\n",
    "    \n",
    "print(X_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 3, 64, 64)\n"
     ]
    }
   ],
   "source": [
    "with open(faces_original_path, 'rb') as file:\n",
    "    X_original = pickle.load(file)\n",
    "    \n",
    "print(X_original.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 3, 64, 64)\n"
     ]
    }
   ],
   "source": [
    "with open(faces_filter_path, 'rb') as file:\n",
    "    X_filter = pickle.load(file)\n",
    "    \n",
    "print(X_filter.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set\n",
      "  image (128, 64, 64, 3)\n",
      "  masked image (128, 64, 64, 3)\n",
      "  mask filter (128, 64, 64, 3)\n",
      "Testing Set\n",
      "  image (100, 64, 64, 3)\n",
      "  masked image (100, 64, 64, 3)\n",
      "  mask filter (100, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "def load_data(X_original_, X_mask_, X_filter_, n_inf, n_sup):\n",
    "    X_image, X_mask, X_filt = X_original_[n_inf:n_sup], X_mask_[n_inf:n_sup], X_filter_[n_inf:n_sup]\n",
    "    X_image = X_image.transpose((0, 2, 3, 1))\n",
    "    X_mask = X_mask.transpose((0, 2, 3, 1))\n",
    "    X_filt = X_filt.transpose((0,2,3,1))\n",
    "    #X_image = X_image[:, :, :, 0]\n",
    "    #X_mask = X_mask[:, :, :, 0]\n",
    "    #X_image = X_image.reshape(X_image.shape[0], -1)\n",
    "    #X_mask = X_mask.reshape(X_mask.shape[0], -1)\n",
    "    return (X_image, X_mask, X_filt)\n",
    "\n",
    "sample_size=128\n",
    "\n",
    "X_train, X_train_mask, X_train_filter = [],[],[]\n",
    "print(\"Training Set\")\n",
    "X_train, X_train_mask, X_train_filter = load_data(X_original, X_mask, X_filter, 0, sample_size)\n",
    "print('  image', X_train.shape)\n",
    "print('  masked image', X_train_mask.shape)\n",
    "print('  mask filter', X_train_filter.shape)\n",
    "\n",
    "print(\"Testing Set\")\n",
    "X_test, X_test_mask, X_test_filter = load_data(X_original, X_mask, X_filter, 1000, 1100)\n",
    "print('  image', X_test.shape)\n",
    "print('  masked image', X_test_mask.shape)\n",
    "print('  mask filter', X_test_filter.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Concat image and filter mask for other nets architecture\\ndef load_data_concat(X_image, X_mask, X_filt):\\n    X_concat = np.concatenate([X_image, X_filt], axis=0)\\n    return X_concat, X_mask\\n\\nX_train, X_train_mask = load_data_concat(X_train, X_train_mask, X_train_filter)\\nX_test, X_test_mask = load_data_concat(X_test, X_test_mask, X_test_filter)\\n\\n\\nprint(\"Training Set\")\\nprint(\\'  image + filter\\', X_train.shape)\\nprint(\\'  masked image\\', X_train_mask.shape)\\n\\nprint(\"Testing Set\")\\nprint(\\'  image + filter\\', X_test.shape)\\nprint(\\'  masked image\\', X_test_mask.shape)'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# Concat image and filter mask for other nets architecture\n",
    "def load_data_concat(X_image, X_mask, X_filt):\n",
    "    X_concat = np.concatenate([X_image, X_filt], axis=0)\n",
    "    return X_concat, X_mask\n",
    "\n",
    "X_train, X_train_mask = load_data_concat(X_train, X_train_mask, X_train_filter)\n",
    "X_test, X_test_mask = load_data_concat(X_test, X_test_mask, X_test_filter)\n",
    "\n",
    "\n",
    "print(\"Training Set\")\n",
    "print('  image + filter', X_train.shape)\n",
    "print('  masked image', X_train_mask.shape)\n",
    "\n",
    "print(\"Testing Set\")\n",
    "print('  image + filter', X_test.shape)\n",
    "print('  masked image', X_test_mask.shape)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (64, 64, 3)\n",
    "n_pixels = int(image_size[0]*image_size[1]*image_size[2])\n",
    "\n",
    "epochs = 30\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.variable_scope(\"approx_net\"):\n",
    "    # user inputs\n",
    "    masked_image_inputs = tf.placeholder(tf.float32, shape=(None,)+image_size)\n",
    "    mask_filter_inputs = tf.placeholder(tf.float32, shape=(None,)+image_size)\n",
    "    real_image_targets = tf.placeholder(tf.float32, shape=(None,)+image_size)\n",
    "    \n",
    "    # reshaping as a long vector (for Dense Net ...)\n",
    "    masked_image_inputs_r = tf.reshape(masked_image_inputs, shape=(-1,n_pixels))\n",
    "    \n",
    "    # First reconstruction net to grossly approximate the masked region\n",
    "    images = tf.layers.dense(masked_image_inputs_r, units=512) \n",
    "    images = tf.nn.leaky_relu(images, alpha=0.2)\n",
    "    \n",
    "    images = tf.layers.dense(images, units=100) \n",
    "    images = tf.nn.leaky_relu(images, alpha=0.2)\n",
    "    \n",
    "    images = tf.layers.dense(images, units=512) \n",
    "    images = tf.nn.leaky_relu(images, alpha=0.2)\n",
    "    \n",
    "    reconstructed_images = tf.layers.dense(images, units=n_pixels, activation=tf.nn.sigmoid)\n",
    "    \n",
    "    # reshaping as a image format again (for Dense Net ...)\n",
    "    reconstructed_images = tf.reshape(reconstructed_images, shape=(-1,)+image_size)\n",
    "    \n",
    "    \n",
    "with tf.variable_scope(\"attention_generator\"):\n",
    "    \n",
    "    attention_input = tf.multiply(mask_filter_inputs, reconstructed_images) \\\n",
    "                    + tf.multiply(1-mask_filter_inputs, masked_image_inputs)  \n",
    "    #attention_input = tf.placeholder(tf.float32, shape=(None,)+image_size)\n",
    "    \n",
    "    attention_input = tf.layers.conv2d(attention_input, filters=18, kernel_size=3, strides=2, padding=\"same\")\n",
    "    attention_input = tf.layers.conv2d(attention_input, filters=18, kernel_size=3, strides=2, padding=\"same\")\n",
    "    \n",
    "    f_branch = tf.layers.conv2d(attention_input, filters=48, kernel_size=1, padding=\"same\")\n",
    "    g_branch = tf.layers.conv2d(attention_input, filters=48, kernel_size=1, padding=\"same\")\n",
    "    \n",
    "    h_branch = tf.layers.conv2d(attention_input, filters=48, kernel_size=1, padding=\"same\")\n",
    "    attention_map = tf.expand_dims(tf.nn.softmax(tf.reduce_sum(tf.multiply(f_branch, g_branch), axis=3)), 3)\n",
    "    #, transpose_a=True)#tf.nn.softmax(tf.matmul(f_branch, g_branch, transpose_a=False))\n",
    "    \n",
    "    attention_output = tf.multiply(attention_map, h_branch)\n",
    "    output_images = tf.layers.conv2d_transpose(attention_output, filters=3, kernel_size=3, strides=2,\n",
    "                                               padding=\"same\", activation=tf.nn.sigmoid)\n",
    "    output_images = tf.layers.conv2d_transpose(output_images, filters=3, kernel_size=3, strides=2,\n",
    "                                               padding=\"same\", activation=tf.nn.sigmoid)\n",
    "    output_images = tf.multiply(mask_filter_inputs, output_images) \\\n",
    "                        + tf.multiply(1-mask_filter_inputs, masked_image_inputs)  \n",
    "    \n",
    "\n",
    "with tf.variable_scope(\"discriminator\"):\n",
    "    \n",
    "    # Labels fake/real\n",
    "    true_labels = tf.placeholder(tf.float32, shape=[None,2])\n",
    "    #discriminator_input = tf.placeholder(tf.float32, shape=(None,)+image_size)\n",
    "    discriminator_input = output_images\n",
    "    \n",
    "    #if training_generator:\n",
    "    #    x = output_images\n",
    "    #else:\n",
    "    #    x = discriminator_input\n",
    "        \n",
    "    x = tf.layers.conv2d(discriminator_input, filters=16, kernel_size=1, strides=2, padding=\"same\", activation=tf.nn.relu)\n",
    "    x = tf.layers.conv2d(x, filters=32, kernel_size=1, strides=2, padding=\"same\", activation=tf.nn.relu)\n",
    "    x = tf.layers.conv2d(x, filters=64, kernel_size=1, strides=2, padding=\"same\", activation=tf.nn.relu)\n",
    "    x = tf.layers.conv2d(x, filters=128, kernel_size=1, strides=2, padding=\"same\", activation=tf.nn.relu)\n",
    "    x = tf.layers.flatten(x)\n",
    "    x = tf.layers.dense(x, units=50, activation=tf.nn.relu)\n",
    "    class_logits = tf.layers.dense(x, units=2)\n",
    "    \n",
    "    \n",
    "with tf.variable_scope('losses'):\n",
    "    # Loss is L2 error on the masked region only\n",
    "    #loss = -1 * tf.reduce_mean(tf.image.ssim(real_image_targets, reconstructed_images, max_val=1.))\n",
    "    #L1_loss = tf.reduce_mean(tf.losses.absolute_difference(real_image_targets, reconstructed_images,\n",
    "    #                                                       scope=mask_filter_inputs==1.))\n",
    "    \n",
    "    reco_loss1 = tf.reduce_mean(tf.nn.l2_loss((real_image_targets - reconstructed_images)*mask_filter_inputs))\n",
    "    #reco_loss1 = tf.losses.absolute_difference(real_image_targets*mask_filter_inputs,reconstructed_images*mask_filter_inputs)\n",
    "    reco_loss2 = tf.reduce_mean(tf.nn.l2_loss(real_image_targets - output_images)) + reco_loss1\n",
    "    #reco_loss2 = tf.losses.absolute_difference(real_image_targets, output_images) + reco_loss1\n",
    "    \n",
    "    perception_loss = -1 * tf.reduce_mean(tf.image.ssim(real_image_targets, reconstructed_images, max_val=1.)) \\\n",
    "                      -1 * tf.reduce_mean(tf.image.ssim(real_image_targets, output_images, max_val=1.))\n",
    "    \n",
    "    # will be fed with both fake/real images only\n",
    "    d_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=true_labels, \n",
    "                                                                    logits=class_logits,\n",
    "                                                                    name=\"d_loss\"))\n",
    "    #d_loss = tf.reduce_mean(true_labels * tf.log(classify) + (1-true_labels)*tf.log(1-classify)) + 0.1*L1_loss\n",
    "    # will be fed with both fake/real images only\n",
    "    g_loss_gan = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=true_labels, \n",
    "                                                                    logits=class_logits,\n",
    "                                                                    name=\"g_loss\"))\n",
    "    g_loss_reco = reco_loss2\n",
    "    #g_loss = 0*g_loss_gan + 1*g_loss_reco\n",
    "    g_loss = perception_loss\n",
    "    \n",
    "with tf.variable_scope('Optimizer'):\n",
    "    \n",
    "    optimizer_g = tf.train.AdamOptimizer(learning_rate=0.002, beta1=0.5)\n",
    "    optimizer_d = tf.train.AdamOptimizer(learning_rate=0.002, beta1=0.5)\n",
    "    \n",
    "    # G Training\n",
    "    vars_g = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"approx_net\") + \\\n",
    "             tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"attention_generator\")\n",
    "    train_op_g = optimizer_g.minimize(g_loss, var_list=vars_g)\n",
    "    \n",
    "    # D Training\n",
    "    vars_d = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"discriminator\")\n",
    "    train_op_d = optimizer_d.minimize(d_loss, var_list=vars_d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /!\\/!\\/!\\/!\\/!\\\n",
    "sess_gan = tf.Session()\n",
    "    \n",
    "sess_gan.run(tf.global_variables_initializer())\n",
    "# /!\\/!\\/!\\/!\\/!\\\n",
    "\n",
    "history = {'loss_g':[], 'loss_d':[], 'loss_perception':[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/Remi/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from ./model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAN model restored.\n",
      "Step 0/30 \ttrain D loss: 0.7014334201812744 \ttrain G loss -1.0313184261322021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|▎         | 1/30 [00:13<06:21, 13.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAN model saved in path: ./model.ckpt\n",
      "Step 1/30 \ttrain D loss: 0.6819546818733215 \ttrain G loss -1.0391685962677002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|▋         | 2/30 [00:26<06:10, 13.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAN model saved in path: ./model.ckpt\n",
      "Step 2/30 \ttrain D loss: 0.6553869247436523 \ttrain G loss -1.057408094406128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 3/30 [00:42<06:20, 14.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAN model saved in path: ./model.ckpt\n",
      "Step 3/30 \ttrain D loss: 0.6338181495666504 \ttrain G loss -1.0653947591781616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 13%|█▎        | 4/30 [01:01<06:42, 15.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAN model saved in path: ./model.ckpt\n",
      "Step 4/30 \ttrain D loss: 0.6519907116889954 \ttrain G loss -1.0601434707641602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|█▋        | 5/30 [01:18<06:39, 16.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAN model saved in path: ./model.ckpt\n",
      "Step 5/30 \ttrain D loss: 0.6799602508544922 \ttrain G loss -1.124180793762207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 6/30 [01:37<06:44, 16.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAN model saved in path: ./model.ckpt\n",
      "Step 6/30 \ttrain D loss: 0.6300697326660156 \ttrain G loss -1.0891705751419067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 23%|██▎       | 7/30 [01:55<06:35, 17.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAN model saved in path: ./model.ckpt\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "saver.restore(sess_gan, \"./model.ckpt\")\n",
    "print(\"GAN model restored.\")\n",
    "from tqdm import *\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    for i in range(X_train.shape[0] // batch_size):\n",
    "        \n",
    "        # boolean, True if we are training G\n",
    "        #training_generator = False\n",
    "        \n",
    "        # load batch_size fake images\n",
    "        idx = np.random.randint(0, sample_size, batch_size)\n",
    "        fake_images_mask = X_train_mask[idx]\n",
    "        fake_images_filter = X_train_filter[idx]\n",
    "        feed_dict = {masked_image_inputs: fake_images_mask,\n",
    "                     mask_filter_inputs: fake_images_filter}\n",
    "        fake_images = sess_gan.run(output_images, feed_dict=feed_dict)\n",
    "        \n",
    "        # load batch_size real images\n",
    "        idx = np.random.randint(0, sample_size, batch_size)\n",
    "        real_images = X_train[idx]\n",
    "        real_images_mask = X_train_mask[idx]\n",
    "        real_images_filter = X_train_filter[idx]\n",
    "        \n",
    "        # concatenate and label fake/real\n",
    "        input_images = np.concatenate([real_images, fake_images])\n",
    "        input_masks = np.concatenate([real_images_mask, fake_images_mask])\n",
    "        input_filters = np.concatenate([real_images_filter, fake_images_filter])\n",
    "        labels = np.zeros((2*batch_size, 2))\n",
    "        labels[batch_size:, 0] = 1.\n",
    "        labels[:batch_size, 1] = 1.\n",
    "        \n",
    "        # Freeze G train Discriminator\n",
    "        feed_dict = {discriminator_input: input_images,\n",
    "                     mask_filter_inputs: input_filters,\n",
    "                     real_image_targets: input_images,\n",
    "                     true_labels: labels}\n",
    "        _, train_loss_d = sess_gan.run([train_op_d, d_loss], feed_dict=feed_dict)\n",
    "\n",
    "        #training_generator = True\n",
    "        \n",
    "        # Freeze D and train Generator\n",
    "        idx = np.random.randint(0, sample_size, batch_size)\n",
    "        labels = np.zeros((batch_size, 2))\n",
    "        labels[:, 1] = 1.\n",
    "        feed_dict = {discriminator_input: X_train_mask[idx], # not matter what\n",
    "                     masked_image_inputs: X_train_mask[idx],\n",
    "                     mask_filter_inputs: X_train_filter[idx],\n",
    "                     real_image_targets: X_train[idx],\n",
    "                     true_labels: labels}\n",
    "        _, train_loss_g = sess_gan.run([train_op_g, g_loss], feed_dict=feed_dict)\n",
    "        \n",
    "        history['loss_g'].append(train_loss_g)\n",
    "        history['loss_d'].append(train_loss_d)\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        feed_dict = {masked_image_inputs: X_test_mask,\n",
    "                     mask_filter_inputs: X_test_filter,\n",
    "                     real_image_targets: X_test}\n",
    "        test_perception_loss = sess_gan.run(perception_loss, feed_dict=feed_dict)\n",
    "        print(\"Step {0}/{1} \\ttrain D loss: {2} \\ttrain G loss {3}\"\\\n",
    "              .format(epoch, epochs, train_loss_d, train_loss_g, test_perception_loss))\n",
    "        history['loss_perception'].append(test_perception_loss)\n",
    "        save_path = saver.save(sess_gan, \"./model.ckpt\")\n",
    "        print(\"GAN model saved in path: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history['loss_d'])\n",
    "plt.title(\"Discriminator Loss\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history['loss_g'])\n",
    "plt.title(\"Generator Loss\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history['loss_perception'])\n",
    "plt.title(\"Perception Loss (Test)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_dict = {masked_image_inputs: X_test_mask,\n",
    "             mask_filter_inputs: X_test_filter,\n",
    "             real_image_targets: X_test}\n",
    "\n",
    "reco_imgs, att_map, out_imgs = sess_gan.run([reconstructed_images, attention_map, output_images], \n",
    "                                             feed_dict=feed_dict)\n",
    "\n",
    "i = 61\n",
    "plt.figure(figsize=(20, 20))\n",
    "\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.imshow(X_test_mask[i])\n",
    "plt.title(\"Masked\")\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.imshow(reco_imgs[i])\n",
    "plt.title(\"Reconstructed\")\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.imshow(X_test_filter[i]*reco_imgs[i] + (1-X_test_filter[i])*X_test_mask[i])\n",
    "plt.title(\"Masked filled\")\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.imshow(out_imgs[i])\n",
    "plt.title(\"Attention GAN output\")\n",
    "plt.subplot(2, 3, 6)\n",
    "plt.imshow(att_map[i].reshape(16, 16), cmap='bwr', interpolation='nearest')\n",
    "plt.title(\"Attention map\")\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.imshow(X_test[i])\n",
    "plt.title(\"Original\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No-GAN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (64, 64, 3)\n",
    "n_pixels = int(image_size[0]*image_size[1]*image_size[2])\n",
    "\n",
    "epochs = 25\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.variable_scope(\"approx_net\"):\n",
    "    # user inputs\n",
    "    masked_image_inputs = tf.placeholder(tf.float32, shape=(None,)+image_size)\n",
    "    mask_filter_inputs = tf.placeholder(tf.float32, shape=(None,)+image_size)\n",
    "    real_image_targets = tf.placeholder(tf.float32, shape=(None,)+image_size)\n",
    "    \n",
    "    # reshaping as a long vector (for Dense Net ...)\n",
    "    masked_image_inputs_r = tf.reshape(masked_image_inputs, shape=(-1,n_pixels))\n",
    "    \n",
    "    # First reconstruction net to grossly approximate the masked region\n",
    "    images = tf.layers.dense(masked_image_inputs_r, units=512) \n",
    "    images = tf.nn.leaky_relu(images, alpha=0.2)\n",
    "    \n",
    "    images = tf.layers.dense(images, units=100) \n",
    "    images = tf.nn.leaky_relu(images, alpha=0.2)\n",
    "    \n",
    "    images = tf.layers.dense(images, units=512) \n",
    "    images = tf.nn.leaky_relu(images, alpha=0.2)\n",
    "    \n",
    "    reconstructed_images = tf.layers.dense(images, units=n_pixels, activation=tf.nn.sigmoid)\n",
    "    \n",
    "    # reshaping as a image format again (for Dense Net ...)\n",
    "    reconstructed_images = tf.reshape(reconstructed_images, shape=(-1,)+image_size)\n",
    "    \n",
    "    \n",
    "with tf.variable_scope(\"attention_generator\"):\n",
    "    \n",
    "    attention_input = tf.multiply(mask_filter_inputs, reconstructed_images) \\\n",
    "                    + tf.multiply(1-mask_filter_inputs, masked_image_inputs)  \n",
    "    #attention_input = tf.placeholder(tf.float32, shape=(None,)+image_size)\n",
    "    \n",
    "    attention_input = tf.layers.conv2d(attention_input, filters=18, kernel_size=3, strides=2, padding=\"same\")\n",
    "    attention_input = tf.layers.conv2d(attention_input, filters=18, kernel_size=3, strides=2, padding=\"same\")\n",
    "    \n",
    "    f_branch = tf.layers.conv2d(attention_input, filters=48, kernel_size=1, padding=\"same\")\n",
    "    g_branch = tf.layers.conv2d(attention_input, filters=48, kernel_size=1, padding=\"same\")\n",
    "    \n",
    "    h_branch = tf.layers.conv2d(attention_input, filters=48, kernel_size=1, padding=\"same\")\n",
    "    attention_map = tf.expand_dims(tf.nn.softmax(tf.reduce_sum(tf.multiply(f_branch, g_branch), axis=3)), 3)\n",
    "    #, transpose_a=True)#tf.nn.softmax(tf.matmul(f_branch, g_branch, transpose_a=False))\n",
    "    \n",
    "    attention_output = tf.multiply(attention_map, h_branch)\n",
    "    output_images = tf.layers.conv2d_transpose(attention_output, filters=3, kernel_size=3, strides=2,\n",
    "                                               padding=\"same\", activation=tf.nn.sigmoid)\n",
    "    output_images = tf.layers.conv2d_transpose(output_images, filters=3, kernel_size=3, strides=2,\n",
    "                                               padding=\"same\", activation=tf.nn.sigmoid)\n",
    "    \n",
    "\n",
    "\"\"\"with tf.variable_scope(\"discriminator\"):\n",
    "    \n",
    "    discriminator_input = tf.multiply(mask_filter_inputs, output_images) \\\n",
    "                        + tf.multiply(1-mask_filter_inputs, masked_image_inputs)  \n",
    "        \n",
    "    x = tf.layers.conv2d(discriminator_input, filters=32, kernel_size=1, strides=2, padding=\"same\", activation=tf.nn.relu)\n",
    "    x = tf.layers.conv2d(x, filters=64, kernel_size=1, strides=2, padding=\"same\", activation=tf.nn.relu)\n",
    "    x = tf.layers.conv2d(x, filters=128, kernel_size=1, strides=2, padding=\"same\", activation=tf.nn.relu)\n",
    "    x = tf.layers.conv2d(x, filters=256, kernel_size=1, strides=2, padding=\"same\", activation=tf.nn.relu)\n",
    "    reshape = tf.reshape(x, [batch_size, -1])\n",
    "    classify = tf.layers.dense(reshape, 2, activation=tf.nn.sigmoid)\"\"\"\n",
    "    \n",
    "    \n",
    "with tf.variable_scope('losses'):\n",
    "    # Loss is L2 error on the masked region only\n",
    "    #loss = -1 * tf.reduce_mean(tf.image.ssim(real_image_targets, reconstructed_images, max_val=1.))\n",
    "    #reco_loss1 = tf.reduce_mean(tf.nn.l2_loss((real_image_targets - reconstructed_images)*mask_filter_inputs))\n",
    "    reco_loss1 = tf.losses.absolute_difference(real_image_targets*mask_filter_inputs,reconstructed_images*mask_filter_inputs)\n",
    "    #reco_loss2 = tf.reduce_mean(tf.nn.l2_loss((real_image_targets - output_images)*mask_filter_inputs)) + reco_loss1\n",
    "    \n",
    "    reco_loss2 = -1 * tf.reduce_mean(tf.image.ssim(real_image_targets, reconstructed_images, max_val=1.)) \\\n",
    "                 -1 * tf.reduce_mean(tf.image.ssim(real_image_targets, output_images, max_val=1.))\n",
    "    \n",
    "with tf.variable_scope('Optimizer'):\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.002, beta1=0.5)\n",
    "    \n",
    "    # To train the approx net only\n",
    "    vars_approx = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"approx_net\")\n",
    "    train_op_approx = optimizer.minimize(reco_loss1, var_list=vars_approx)\n",
    "    # To train the attention only\n",
    "    vars_attention = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"attention_generator\")\n",
    "    train_op_attention = optimizer.minimize(reco_loss2)#, var_list=vars_attention)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /!\\/!\\/!\\/!\\/!\\\n",
    "sess = tf.Session()\n",
    "    \n",
    "sess.run(tf.global_variables_initializer())\n",
    "# /!\\/!\\/!\\/!\\/!\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    for i in range(X_train.shape[0] // batch_size):\n",
    "        # Draw a random batch and train on it\n",
    "        idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "        feed_dict = {masked_image_inputs: X_train_mask[idx],\n",
    "                     mask_filter_inputs: X_train_filter[idx],\n",
    "                     real_image_targets: X_train[idx]}\n",
    "        _, train_loss = sess.run([train_op_approx, reco_loss1], feed_dict=feed_dict)\n",
    "        #_, train_loss = sess.run([train_op_attention, reco_loss2], feed_dict=feed_dict)\n",
    "    if epoch % 1 == 0:\n",
    "        feed_dict = {masked_image_inputs: X_test_mask,\n",
    "                     mask_filter_inputs: X_test_filter,\n",
    "                     real_image_targets: X_test}\n",
    "        test_loss = sess.run(reco_loss1, feed_dict=feed_dict)\n",
    "        print(\"Step {0}/{1} \\ttrain loss: {2} \\ttest loss {3}\".format(epoch, epochs, train_loss, test_loss))\n",
    "        #if epoch % 10 == 0:\n",
    "        #    reco_imgs = sess.run(reconstructed_images, feed_dict=feed_dict)\n",
    "        #    i = 3\n",
    "        #    plt.imshow(X_test_mask[i])\n",
    "        #    plt.show()\n",
    "        #    plt.imshow(reco_imgs[i])\n",
    "        #    plt.show()\n",
    "        #    plt.imshow(X_test_filter[i]*reco_imgs[i] + (1-X_test_filter[i])*X_test_mask[i])\n",
    "        #    plt.show()\n",
    "        #    plt.imshow(X_test[i])\n",
    "        #   plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_dict = {masked_image_inputs: X_test_mask,\n",
    "             mask_filter_inputs: X_test_filter,\n",
    "             real_image_targets: X_test}\n",
    "\n",
    "reco_imgs, att_map, out_imgs = sess.run([reconstructed_images, attention_map, output_images], \n",
    "                                        feed_dict=feed_dict)\n",
    "\n",
    "i = 99\n",
    "plt.imshow(X_test_mask[i])\n",
    "plt.title(\"Masked\")\n",
    "plt.show()\n",
    "plt.imshow(reco_imgs[i])\n",
    "plt.title(\"Reconstructed\")\n",
    "plt.show()\n",
    "plt.imshow(X_test_filter[i]*reco_imgs[i] + (1-X_test_filter[i])*X_test_mask[i])\n",
    "plt.title(\"Masked filled\")\n",
    "plt.show()\n",
    "plt.imshow(X_test_filter[i]*out_imgs[i] + (1-X_test_filter[i])*X_test_mask[i])\n",
    "plt.title(\"Attention GAN output\")\n",
    "plt.show()\n",
    "plt.imshow(att_map[i].reshape(16, 16), cmap='bwr', interpolation='nearest')\n",
    "plt.title(\"Attention map\")\n",
    "plt.show()\n",
    "plt.imshow(X_test[i])\n",
    "plt.title(\"Original\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_dict = {masked_image_inputs: X_test_mask,\n",
    "             mask_filter_inputs: X_test_filter,\n",
    "             real_image_targets: X_test}\n",
    "\n",
    "reco_imgs, att_map, out_imgs = sess.run([reconstructed_images, attention_map, output_images], \n",
    "                                        feed_dict=feed_dict)\n",
    "\n",
    "i = 5\n",
    "plt.figure(figsize=(20, 20))\n",
    "\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.imshow(X_test_mask[i])\n",
    "plt.title(\"Masked\")\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.imshow(reco_imgs[i])\n",
    "plt.title(\"Reconstructed\")\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.imshow(X_test_filter[i]*reco_imgs[i] + (1-X_test_filter[i])*X_test_mask[i])\n",
    "plt.title(\"Masked filled\")\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.imshow(X_test_filter[i]*out_imgs[i] + (1-X_test_filter[i])*X_test_mask[i])\n",
    "plt.title(\"Attention GAN output\")\n",
    "plt.subplot(2, 3, 6)\n",
    "plt.imshow(att_map[i].reshape(16, 16), cmap='bwr', interpolation='nearest')\n",
    "plt.title(\"Attention map\")\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.imshow(X_test[i])\n",
    "plt.title(\"Original\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_dict2 = {attention_input: reconstructed_images}\n",
    "\n",
    "sess.run(output_images, feed_dict=feed_dict2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sandbox Zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#img1 = tf.Variable(X_train[0].transpose(1, 2, 0))\n",
    "#img2 = tf.Variable(X_train[0].transpose(1, 2, 0))\n",
    "img1 = tf.placeholder(tf.float32, shape=(64, 64, 3))\n",
    "img2 = tf.placeholder(tf.float32, shape=(64, 64, 3))\n",
    "max_val = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = tf.image.ssim(img1, img2, max_val=1.)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    #print(sess.run(similarity, feed_dict={img1:X_train[0], img2:X_train_mask[0]}))\n",
    "    print(sess.run(-similarity, feed_dict={img1:X_test_filter[i]*out_imgs[i] + (1-X_test_filter[i])*X_test_mask[i], \n",
    "                                          img2:X_test[i]}))\n",
    "\n",
    "plt.imshow(X_test[i])\n",
    "plt.show()\n",
    "plt.imshow(X_test_filter[i]*out_imgs[i] + (1-X_test_filter[i])*X_test_mask[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    z = tf.image.extract_image_patches(tf.expand_dims(img1, 0), \n",
    "                                       [1, 8, 8, 1], \n",
    "                                       [1, 8, 8, 1], \n",
    "                                       [1,1,1,1], \n",
    "                                       padding=\"SAME\", \n",
    "                                       name=None)\n",
    "    print(z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels2_test = np.array([[1., 1., 0., 0.]])\n",
    "class_logits2_test = np.array([[60., 60., 14., 14.]])\n",
    "sess_test = tf.Session()\n",
    "sess_test.run(tf.nn.softmax_cross_entropy_with_logits(labels=true_labels2_test, logits=class_logits2_test, name=\"test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels2_test = np.array([[0., 1.],\n",
    "                              [0., 1.], \n",
    "                              [0., 1.], \n",
    "                              [0., 1.]])\n",
    "class_logits2_test = np.array([[20., 18.],\n",
    "                              [2., 19.], \n",
    "                              [1., 21.], \n",
    "                              [1., 45.]])\n",
    "sess_test = tf.Session()\n",
    "sess_test.run(tf.nn.softmax_cross_entropy_with_logits(labels=true_labels2_test, logits=class_logits2_test, name=\"test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------- DISCRIMINATOR 2 ----------------- # \n",
    "\n",
    "with tf.variable_scope(\"discriminator2\"):\n",
    "    \n",
    "    # Labels fake/real\n",
    "    true_labels2 = tf.placeholder(tf.float32, shape=[None,2])\n",
    "    discriminator_input2 = tf.placeholder(tf.float32, shape=(None,)+image_size)\n",
    "        \n",
    "    x2 = tf.layers.conv2d(discriminator_input2, filters=32, kernel_size=1, strides=2, padding=\"same\", activation=tf.nn.relu)\n",
    "    x2 = tf.layers.conv2d(x2, filters=32, kernel_size=1, strides=2, padding=\"same\", activation=tf.nn.relu)\n",
    "    x2 = tf.layers.conv2d(x2, filters=64, kernel_size=1, strides=2, padding=\"same\", activation=tf.nn.relu)\n",
    "    x2 = tf.layers.conv2d(x2, filters=128, kernel_size=1, strides=2, padding=\"same\", activation=tf.nn.relu)\n",
    "    x2 = tf.layers.flatten(x2)\n",
    "    x2 = tf.layers.dense(x2, units=50, activation=tf.nn.relu)\n",
    "    #class_logits = tf.layers.dense(x, units=1, activation=tf.nn.sigmoid)\n",
    "    class_logits2 = tf.layers.dense(x2, units=2)\n",
    "    #class_logits2 = tf.squeeze(class_logits2, axis=1)\n",
    "    \n",
    "    # Loss function\n",
    "    d_loss2 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=true_labels2, \n",
    "                                                                     logits=class_logits2,\n",
    "                                                                     name=\"d_loss2\"))\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer_d2 = tf.train.AdamOptimizer(learning_rate=0.002, beta1=0.5)\n",
    "    \n",
    "    # Op\n",
    "    train_op_d2 = optimizer_d2.minimize(d_loss2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "batch_size = 64\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range(X_train.shape[0] // batch_size):\n",
    "        # load batch_size fake images\n",
    "        #idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "        #fake_images_mask = X_train_mask[idx]\n",
    "        #fake_images_filter = X_train_filter[idx]\n",
    "        #feed_dict = {masked_image_inputs: fake_images_mask,\n",
    "        #             mask_filter_inputs: fake_images_filter}\n",
    "        #fake_images = sess_gan.run(discriminator_input, feed_dict=feed_dict)\n",
    "        idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "        fake_images = X_train_mask[idx] # take masked images\n",
    "        \n",
    "        # load batch_size real images\n",
    "        idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "        real_images = X_train[idx]\n",
    "        \n",
    "        # concatenate and label fake/real\n",
    "        input_images = np.concatenate([real_images, fake_images])\n",
    "        #labels = np.concatenate([np.ones(batch_size), np.zeros(batch_size)])\n",
    "        labels = np.zeros((2*batch_size, 2))\n",
    "        labels[batch_size:, 0] = 1.\n",
    "        labels[:batch_size, 1] = 1.\n",
    "        \n",
    "        # Freeze G train Discriminator\n",
    "        feed_dict = {discriminator_input2: input_images,\n",
    "                     true_labels2: labels}\n",
    "        _, train_loss_d2 = sess_gan.run([train_op_d2, d_loss2], feed_dict=feed_dict)\n",
    "        \n",
    "        #print(sess_gan.run(class_logits2, feed_dict=feed_dict))\n",
    "        \n",
    "    if epoch % 1 == 0:\n",
    "        print(\"Step {0}/{1} \\ttrain D loss: {2}\"\\\n",
    "              .format(epoch, epochs, train_loss_d2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
